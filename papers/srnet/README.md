# Paper Reading
- [Editing Text in the Wild](https://arxiv.org/pdf/1908.03047v1.pdf)
## Introduction
- Figure 1
    - Given a text image, our goal is to replace the text instance in it without damaging its realistic look. As in (a), the proposed scene text editor produces realistic text images by editing each word in the source image, retaining the styles of both the text and background.
    - As (b), there are two major challenges for scene text editing: text style transfer and background texture retention. Specially, the text style consists of diverse factors such as language, font, color, orientation, stroke size and spatial perspective, which makes it hard to precisely capture the complete text style in source image and transfer them to the target text. Meanwhile, it is also difficult to maintain the consistency of the edited background, especially when text appears on some complex scenes, such as menu and street store sign. Moreover, if the target text is shorter than the original text, the exceeding region of characters should be erased and filled with appropriate texture.
- Considering these challenges, we propose a style retention network (SRNet) for scene text editing which learns from pairs of images. The core idea of SRNet is to decompose the complex task into several simpler, modular and joint-trainable sub networks: text conversion module, background inpainting module and fusion mod- ule, as illustrated in Fig. 2. Firstly, the text conversion module (TCM).
## Related Works
- Character-level methods
    - When dealing with words of different lengths, our word-level editor can adjust the placement of foreground characters adaptively, while character-level methods ignore.
## Methodology
- Text Erasure and Editing
    - Background texture needs to be consistent with that before editing for scene text editing. There are some related works of text erasure, trying to erase the scene text stroke pixels while completing image inpainting on corresponding positions.
    - [21] proposed an image-patch based framework for text erasure, but large computational cost is induced due to the sliding window based processing mechanism.
    - EnsNet [35] firstly introduced the generative adversarial network to text erasing, which can erase the scene text on the whole image in an end-to-end manner. With the help of refined loss, the visualization results looks better than those of pix2pix [11].
    - In the process of text editing, ***we only pay attention to background erasure at word-level, therefore, the background inpainting module in SRNet can be designed more light and still have good erasure performance.***
    - We noticed that a recent paper [24] try to study the issue of scene text editing, but it can only transfer the color and font of a single character in one process while ignoring the consistency of background texture.
- We present a style retention network (SRNet) for scene text editing. During training, the SRNet takes as input a pair of images (Is , It ) where Is is the source style image and It is the target text image. The outputs ((Tsk ,Tt ),Tb ,Tf ) where Tsk is the target text skeleton, Tt is the foreground image which has the same text style as Is . Tb is the background of Is and Tf is the final target text image. In order to effectively tackle the two major challenges mentioned in Sec. 1, we decompose the SRNet into three simpler and learnable sub networks: 1) text conversion module, 2) background inpainting module and 3) fusion module, as illustrated in Fig. 2. Specifically, the text style from source image Is is transferred to the target text with the help of a skeleton-guided learning mechanism aiming to retain text semantics(Sec. 3.1). Meanwhile the background information is filled by learning an erasure or inpainting task (Sec. 3.2). Lastly, the transferred target image and completed background are fused by the text fusion network, generating the edited image
- Text Conversion Module
- We render the target text into a standard image with a fixed font and background pixel value setting to 127, and the rendered im- age is denoted as target text image It . The text conversion module (blue part in Fig. 2) takes the source image Is and the target text image It as inputs, and aims to extract the foreground style from the source image Is and transfers it to the target text image It . In particular, the foreground style contains text style, including font, color, geometric deformation, and so on. Thus, the text conversion module outputs an image Ot which has the semantics of the target text and the text style of the source image. An encoder-decoder FCN is adopted in this work. For encoding, the source image Is is encoded by 3 down-sampling convolutional layers and 4 residual blocks [9], the input text image It is also encoded by the same archi- tecture, then two features are concatenated along their depth axis. For decoding, there are 3 up-sampling transposed convolutional lay- ers and 1 Convolution-BatchNorm-LeakyReLU blocks to generate the output Ot . Moreover, we introduce a skeleton-guided learning mechanism to generate more robust text. We use GT to denote the text conversion module and the output can be represented as: Ot = GT (It , Is ). (1)
- transfers the text style of the source image to the target text, includ- ing font, color, position, and scale. In order to keep the semantics of the target text, we introduce a skeleton-guided learning mechanism to the TCM, whose effectiveness has been verified in Exp. 4.4. At the same time, the background inpainting module (BIM) erases the original text stroke pixels and fills them with appropriate texture in a bottom-up feature fusion manner, following the general archi- tecture of a "U-Net" [23]. Finally, the fusion module automatically learns how to fuse foreground information and background texture information effectively, so as to synthesize edited text image.
- the proposed SRNet decomposes the net- work into modular sub networks, while decomposes the complex task into several easy-to-learn tasks.
- To our knowledge, this work is the first to address the prob- lem of word or text-line level scene text editing by an end- to-end trainable network;
- Under the guidance of stroke skeleton, the proposed network can keep the semantic information as much as possible; â€¢ The proposed method exhibits superior performance on sev- eral scene text editing tasks like intra-language text image editing, AR translation (cross-language), information hiding (e.g. word-level text erasure), etc.
## References
- [5] [TranslatAR: A Mobile Augmented Reality Translator](http://vfragoso.com/pdfs/131.pdf)
- [11] [pix2pix]
- [21] [Scene text eraser]
- [24] [STEFANN: Scene Text Editor using Font Adaptive Neural Network]
- [33] [Context-Aware Unsupervised Text Stylization](http://39.96.165.147/Pub%20Files/2018/yang-mm18.pdf)
- [35] [Ensnet: Ensconce text in the wild]

# Repositories
- https://github.com/Niwhskal/SRNet
